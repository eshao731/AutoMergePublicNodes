#!/usr/bin/env python3
# pyright: reportConstantRedefinition = none
# pyright: reportMissingTypeStubs = none
# pyright: reportRedeclaration = none
# pyright: reportMissingParameterType = none
# pyright: reportUnnecessaryIsInstance = none
# pyright: reportUnknownVariableType = none
# pyright: reportUnknownMemberType = none
# pyright: reportUnknownArgumentType = none
# pyright: reportArgumentType = none
# pyright: reportAttributeAccessIssue = none
# pyright: reportGeneralTypeIssues = none
import yaml
import json
import base64
from urllib.parse import quote, unquote, urlparse
import requests
from requests_file import FileAdapter
import datetime
import traceback
import binascii
import threading
import sys
import os
import copy
import socket
import time
import subprocess
import tempfile
import shutil
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from types import FunctionType as function
from typing import Set, List, Dict, Tuple, Union, Callable, Any, Optional, no_type_check

# æºå†å²è®°å½•ç›¸å…³å¸¸é‡
SOURCE_HISTORY_FILE = "source_history.json"
SOURCE_DELETE_FILE = "source_delete.list"
SOURCES_FILE = "sources.list"
INVALID_DAYS_THRESHOLD = 7  # è¿ç»­æ— æ•ˆå¤©æ•°é˜ˆå€¼

try: PROXY = open("local_proxy.conf").read().strip()
except FileNotFoundError: LOCAL = False; PROXY = None
else:
    if not PROXY: PROXY = None
    LOCAL = not PROXY

def b64encodes(s: str):
    return base64.b64encode(s.encode('utf-8')).decode('utf-8')

def b64encodes_safe(s: str):
    return base64.urlsafe_b64encode(s.encode('utf-8')).decode('utf-8')

def b64decodes(s: str):
    ss = s + '=' * ((4-len(s)%4)%4)
    try:
        return base64.b64decode(ss.encode('utf-8')).decode('utf-8')
    except UnicodeDecodeError: raise
    except binascii.Error: raise

def b64decodes_safe(s: str):
    ss = s + '=' * ((4-len(s)%4)%4)
    try:
        return base64.urlsafe_b64decode(ss.encode('utf-8')).decode('utf-8')
    except UnicodeDecodeError: raise
    except binascii.Error: raise

DEFAULT_UUID = '8'*8+'-8888'*3+'-'+'8'*12

CLASH2VMESS = {'name': 'ps', 'server': 'add', 'port': 'port', 'uuid': 'id', 
              'alterId': 'aid', 'cipher': 'scy', 'network': 'net', 'servername': 'sni'}
VMESS2CLASH: Dict[str, str] = {}
for k,v in CLASH2VMESS.items(): VMESS2CLASH[v] = k

VMESS_EXAMPLE = {
    "v": "2", "ps": "", "add": "0.0.0.0", "port": "0", "aid": "0", "scy": "auto",
    "net": "tcp", "type": "none", "tls": "", "id": DEFAULT_UUID
}

CLASH_CIPHER_VMESS = "auto aes-128-gcm chacha20-poly1305 none".split()
CLASH_CIPHER_SS = "aes-128-gcm aes-192-gcm aes-256-gcm aes-128-cfb aes-192-cfb \
        aes-256-cfb aes-128-ctr aes-192-ctr aes-256-ctr rc4-md5 chacha20-ietf \
        xchacha20 chacha20-ietf-poly1305 xchacha20-ietf-poly1305".split()
CLASH_SSR_OBFS = "plain http_simple http_post random_head tls1.2_ticket_auth tls1.2_ticket_fastauth".split()
CLASH_SSR_PROTOCOL = "origin auth_sha1_v4 auth_aes128_md5 auth_aes128_sha1 auth_chain_a auth_chain_b".split()

ABFURLS = (
    "https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/ChineseFilter/sections/adservers.txt",
    "https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/ChineseFilter/sections/adservers_firstparty.txt",
    "https://raw.githubusercontent.com/AdguardTeam/FiltersRegistry/master/filters/filter_224_Chinese/filter.txt",
    # "https://raw.githubusercontent.com/AdguardTeam/FiltersRegistry/master/filters/filter_15_DnsFilter/filter.txt",
    # "https://malware-filter.gitlab.io/malware-filter/urlhaus-filter-ag.txt",
    # "https://raw.githubusercontent.com/banbendalao/ADgk/master/ADgk.txt",
    # "https://raw.githubusercontent.com/hoshsadiq/adblock-nocoin-list/master/nocoin.txt",
    # "https://anti-ad.net/adguard.txt",
    "https://raw.githubusercontent.com/TG-Twilight/AWAvenue-Ads-Rule/main/AWAvenue-Ads-Rule.txt",
    "https://raw.githubusercontent.com/d3ward/toolz/master/src/d3host.adblock",
    # "https://raw.githubusercontent.com/Cats-Team/AdRules/main/dns.txt",
    # "https://raw.githubusercontent.com/hagezi/dns-blocklists/main/adblock/light.txt",
    # "https://raw.githubusercontent.com/uniartisan/adblock_list/master/adblock_lite.txt",
    "https://raw.githubusercontent.com/afwfv/DD-AD/main/rule/DD-AD.txt",
    # "https://raw.githubusercontent.com/afwfv/DD-AD/main/rule/domain.txt",
)

ABFWHITE = (
    "https://raw.githubusercontent.com/privacy-protection-tools/dead-horse/master/anti-ad-white-list.txt",
    "file:///abpwhite.txt",
)

FAKE_IPS = "8.8.8.8; 8.8.4.4; 4.2.2.2; 4.2.2.1; 114.114.114.114; 127.0.0.1; 0.0.0.0".split('; ')
FAKE_DOMAINS = ".google.com .github.com".split()

FETCH_TIMEOUT = (6, 5)

BANNED_WORDS = b64decodes('5rOV6L2uIOi9ruWtkCDova4g57uDIOawlCDlip8gb25ndGFpd2Fu').split()

# !!! JUST FOR DEBUGING !!!
DEBUG_NO_NODES = os.path.exists("local_NO_NODES")
DEBUG_NO_DYNAMIC = os.path.exists("local_NO_DYNAMIC")
DEBUG_NO_ADBLOCK = os.path.exists("local_NO_ADBLOCK")

STOP = False
STOP_FAKE_NODES = """vmess://ew0KICAidiI6ICIyIiwNCiAgInBzIjogIlx1NUU4Nlx1Nzk1RFx1NEU5QVx1NTFBQ1x1NEYxQVx1ODBEQ1x1NTIyOVx1NTNFQ1x1NUYwMCIsDQogICJhZGQiOiAid2ViLjUxLmxhIiwNCiAgInBvcnQiOiAiNDQzIiwNCiAgImlkIjogIjg4ODg4ODg4LTg4ODgtODg4OC04ODg4LTg4ODg4ODg4ODg4OCIsDQogICJhaWQiOiAiMCIsDQogICJzY3kiOiAiYXV0byIsDQogICJuZXQiOiAidGNwIiwNCiAgInR5cGUiOiAiaHR0cCIsDQogICJob3N0IjogIndlYi41MS5sYSIsDQogICJwYXRoIjogIi9pbWFnZXMvaW5kZXgvc2VydmljZS1waWMucG5nIiwNCiAgInRscyI6ICJ0bHMiLA0KICAic25pIjogIndlYi41MS5sYSIsDQogICJhbHBuIjogImh0dHAvMS4xIiwNCiAgImZwIjogImNocm9tZSINCn0=
vmess://ew0KICAidiI6ICIyIiwNCiAgInBzIjogIlx1NjU0Rlx1NjExRlx1NjVGNlx1NjcxRlx1RkYwQ1x1NjZGNFx1NjVCMFx1NjY4Mlx1NTA1QyIsDQogICJhZGQiOiAid2ViLjUxLmxhIiwNCiAgInBvcnQiOiAiNDQzIiwNCiAgImlkIjogImM2ZTg0MDcyLTJlNjktNDkyOC05MGFmLTQzNmIzZmNkMDY2MyIsDQogICJhaWQiOiAiMCIsDQogICJzY3kiOiAiYXV0byIsDQogICJuZXQiOiAidGNwIiwNCiAgInR5cGUiOiAiaHR0cCIsDQogICJob3N0IjogIndlYi41MS5sYSIsDQogICJwYXRoIjogIi9pbWFnZXMvaW5kZXgvc2VydmljZS1waWMucG5nIiwNCiAgInRscyI6ICJ0bHMiLA0KICAic25pIjogIndlYi41MS5sYSIsDQogICJhbHBuIjogImh0dHAvMS4xIiwNCiAgImZwIjogImNocm9tZSINCn0=
vmess://ew0KICAidiI6ICIyIiwNCiAgInBzIjogIlx1NTk4Mlx1NjcwOVx1OTcwMFx1ODk4MVx1RkYwQ1x1ODFFQVx1ODg0Q1x1NjQyRFx1NUVGQSIsDQogICJhZGQiOiAid2ViLjUxLmxhIiwNCiAgInBvcnQiOiAiNDQzIiwNCiAgImlkIjogImUwYzZiM2I3LTlmNWItNGJkNi05YWJmLTI2MDY2M2FhNGYxYiIsDQogICJhaWQiOiAiMCIsDQogICJzY3kiOiAiYXV0byIsDQogICJuZXQiOiAidGNwIiwNCiAgInR5cGUiOiAiaHR0cCIsDQogICJob3N0IjogIndlYi41MS5sYSIsDQogICJwYXRoIjogIi9pbWFnZXMvaW5kZXgvc2VydmljZS1waWMucG5nIiwNCiAgInRscyI6ICJ0bHMiLA0KICAic25pIjogIndlYi41MS5sYSIsDQogICJhbHBuIjogImh0dHAvMS4xIiwNCiAgImZwIjogImNocm9tZSINCn0=
"""

class UnsupportedType(Exception): pass
class NotANode(Exception): pass

session = requests.Session()
session.trust_env = False
if PROXY: session.proxies = {'http': PROXY, 'https': PROXY}
session.headers["User-Agent"] = 'Mozilla/5.0 (X11; Linux x86_64) Clash-verge/v2.0.3 AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.58'
session.mount('file://', FileAdapter())
    
exc_queue: List[str] = []

d = datetime.datetime.now()
if STOP or (d.month, d.day) in ((6, 4), (7, 1), (10, 1)):
    DEBUG_NO_NODES = DEBUG_NO_DYNAMIC = STOP = True

class Node:
    names: Set[str] = set()
    DATA_TYPE = Dict[str, Any]

    def __init__(self, data: Union[DATA_TYPE, str]) -> None:
        if isinstance(data, dict):
            self.data: __class__.DATA_TYPE = data
            self.type = data['type']
        elif isinstance(data, str):
            self.load_url(data)
        else: raise TypeError(f"Got {type(data)}")
        if not self.data['name']:
            self.data['name'] = "æœªå‘½å"
        if 'password' in self.data:
            self.data['password'] = str(self.data['password'])
        self.data['type'] = self.type
        self.name: str = self.data['name']

    def __str__(self):
        return self.url

    def __hash__(self):
        data = self.data
        try:
            path = ""
            if self.type == 'vmess':
                net: str = data.get('network', '')
                path = net+':'
                if not net: pass
                elif net == 'ws':
                    opts: Dict[str, Any] = data.get('ws-opts', {})
                    path += opts.get('headers', {}).get('Host', '')
                    path += '/'+opts.get('path', '')
                elif net == 'h2':
                    opts: Dict[str, Any] = data.get('h2-opts', {})
                    path += ','.join(opts.get('host', []))
                    path += '/'+opts.get('path', '')
                elif net == 'grpc':
                    path += data.get('grpc-opts', {}).get('grpc-service-name','')
            elif self.type == 'ss':
                opts: Dict[str, Any] = data.get('plugin-opts', {})
                path = opts.get('host', '')
                path += '/'+opts.get('path', '')
            elif self.type == 'ssr':
                path = data.get('obfs-param', '')
            elif self.type == 'trojan':
                path = data.get('sni', '')+':'
                net: str = data.get('network', '')
                if not net: pass
                elif net == 'ws':
                    opts: Dict[str, Any] = data.get('ws-opts', {})
                    path += opts.get('headers', {}).get('Host', '')
                    path += '/'+opts.get('path', '')
                elif net == 'grpc':
                    path += data.get('grpc-opts', {}).get('grpc-service-name','')
            elif self.type == 'vless':
                path = data.get('sni', '')+':'
                net: str = data.get('network', '')
                if not net: pass
                elif net == 'ws':
                    opts: Dict[str, Any] = data.get('ws-opts', {})
                    path += opts.get('headers', {}).get('Host', '')
                    path += '/'+opts.get('path', '')
                elif net == 'grpc':
                    path += data.get('grpc-opts', {}).get('grpc-service-name','')
            elif self.type == 'hysteria2':
                path = data.get('sni', '')+':'
                path += data.get('obfs-password', '')+':'
                # print(self.url)
                # return hash(self.url)
            path += '@'+','.join(data.get('alpn', []))+'@'+data.get('password', '')+data.get('uuid', '')
            hashstr = f"{self.type}:{data['server']}:{data['port']}:{path}"
            return hash(hashstr)
        except Exception:
            print("èŠ‚ç‚¹ Hash è®¡ç®—å¤±è´¥ï¼", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
            return hash('__ERROR__')
    
    def __eq__(self, other: Union['Node', Any]):
        if isinstance(other, self.__class__):
            return hash(self) == hash(other)
        else:
            return False

    def load_url(self, url: str) -> None:
        try: self.type, dt = url.split("://", 1)
        except ValueError: raise NotANode(url)
        # === Fix begin ===
        if not self.type.isascii():
            self.type = ''.join([_ for _ in self.type if _.isascii()])
            url = self.type+'://'+url.split("://")[1]
        if self.type == 'hy2': self.type = 'hysteria2'
        # === Fix end ===
        if self.type == 'vmess':
            v = VMESS_EXAMPLE.copy()
            try: v.update(json.loads(b64decodes(dt)))
            except Exception:
                raise UnsupportedType('vmess', 'SP')
            self.data = {}
            for key, val in v.items():
                if key in VMESS2CLASH:
                    self.data[VMESS2CLASH[key]] = val
            self.data['tls'] = (v['tls'] == 'tls')
            # å®‰å…¨åœ°è½¬æ¢ alterIdï¼Œå¤„ç†æ— æ•ˆå€¼
            try:
                self.data['alterId'] = int(self.data['alterId'])
            except (ValueError, KeyError):
                self.data['alterId'] = 0  # é»˜è®¤å€¼ä¸º 0
            if v['net'] == 'ws':
                opts = {}
                if 'path' in v:
                    opts['path'] = v['path']
                if 'host' in v:
                    opts['headers'] = {'Host': v['host']}
                self.data['ws-opts'] = opts
            elif v['net'] == 'h2':
                opts = {}
                if 'path' in v:
                    opts['path'] = v['path']
                if 'host' in v:
                    opts['host'] = v['host'].split(',')
                self.data['h2-opts'] = opts
            elif v['net'] == 'grpc' and 'path' in v:
                self.data['grpc-opts'] = {'grpc-service-name': v['path']}

        elif self.type == 'ss':
            info = url.split('@')
            srvname = info.pop()
            if '#' in srvname:
                srv, name = srvname.split('#')
            else:
                srv = srvname
                name = ''
            server, port = srv.split(':')
            try:
                port = int(port)
            except ValueError:
                raise UnsupportedType('ss', 'SP')
            info = '@'.join(info)
            if not ':' in info:
                info = b64decodes_safe(info)
            if ':' in info:
                cipher, passwd = info.split(':', 1)  # ä½¿ç”¨ maxsplit=1 æ¥å¤„ç†å¯†ç ä¸­åŒ…å« : çš„æƒ…å†µ
            else:
                cipher = info
                passwd = ''
            self.data = {'name': unquote(name), 'server': server,
                    'port': port, 'type': 'ss', 'password': passwd, 'cipher': cipher}

        elif self.type == 'ssr':
            if '?' in url:
                parts = dt.split(':')
            else:
                parts = b64decodes_safe(dt).split(':')
            try:
                passwd, info = parts[-1].split('/?')
            except: raise
            passwd = b64decodes_safe(passwd)
            self.data = {'type': 'ssr', 'server': parts[0], 'port': parts[1],
                    'protocol': parts[2], 'cipher': parts[3], 'obfs': parts[4],
                    'password': passwd, 'name': ''}
            for kv in info.split('&'):
                k_v = kv.split('=')
                if len(k_v) != 2:
                    k = k_v[0]
                    v = ''
                else: k,v = k_v
                if k == 'remarks':
                    self.data['name'] = v
                elif k == 'group':
                    self.data['group'] = v
                elif k == 'obfsparam':
                    self.data['obfs-param'] = v
                elif k == 'protoparam':
                    self.data['protocol-param'] = v

        elif self.type == 'trojan':
            parsed = urlparse(url)
            self.data = {'name': unquote(parsed.fragment), 'server': parsed.hostname, 
                    'port': parsed.port, 'type': 'trojan', 'password': unquote(parsed.username)} # type: ignore
            if parsed.query:
                for kv in parsed.query.split('&'):
                    k,v = kv.split('=')
                    if k in ('allowInsecure', 'insecure'):
                        self.data['skip-cert-verify'] = (v != '0')
                    elif k == 'sni': self.data['sni'] = v
                    elif k == 'alpn':
                        self.data['alpn'] = unquote(v).split(',')
                    elif k == 'type':
                        self.data['network'] = v
                    elif k == 'serviceName':
                        if 'grpc-opts' not in self.data:
                            self.data['grpc-opts'] = {}
                        self.data['grpc-opts']['grpc-service-name'] = v
                    elif k == 'host':
                        if 'ws-opts' not in self.data:
                            self.data['ws-opts'] = {}
                        if 'headers' not in self.data['ws-opts']:
                            self.data['ws-opts']['headers'] = {}
                        self.data['ws-opts']['headers']['Host'] = v
                    elif k == 'path':
                        if 'ws-opts' not in self.data:
                            self.data['ws-opts'] = {}
                        self.data['ws-opts']['path'] = v

        elif self.type == 'vless':
            parsed = urlparse(url)
            self.data = {'name': unquote(parsed.fragment), 'server': parsed.hostname, 
                    'port': parsed.port, 'type': 'vless', 'uuid': unquote(parsed.username)} # type: ignore
            self.data['tls'] = False
            if parsed.query:
                for kv in parsed.query.split('&'):
                    if '=' not in kv:
                        continue
                    k, v = kv.split('=', 1)  # ä½¿ç”¨ maxsplit=1 æ¥å¤„ç†å€¼ä¸­åŒ…å« = çš„æƒ…å†µ
                    if k in ('allowInsecure', 'insecure'):
                        self.data['skip-cert-verify'] = (v != '0')
                    elif k == 'sni': self.data['servername'] = v
                    elif k == 'alpn':
                        self.data['alpn'] = unquote(v).split(',')
                    elif k == 'type':
                        self.data['network'] = v
                    elif k == 'serviceName':
                        if 'grpc-opts' not in self.data:
                            self.data['grpc-opts'] = {}
                        self.data['grpc-opts']['grpc-service-name'] = v
                    elif k == 'host':
                        if 'ws-opts' not in self.data:
                            self.data['ws-opts'] = {}
                        if 'headers' not in self.data['ws-opts']:
                            self.data['ws-opts']['headers'] = {}
                        self.data['ws-opts']['headers']['Host'] = v
                    elif k == 'path':
                        if 'ws-opts' not in self.data:
                            self.data['ws-opts'] = {}
                        self.data['ws-opts']['path'] = v
                    elif k == 'flow':
                        if v.endswith('-udp443'):
                            self.data['flow'] = v
                        else: self.data['flow'] = v+'!'
                    elif k == 'fp': self.data['client-fingerprint'] = v
                    elif k == 'security' and v == 'tls':
                        self.data['tls'] = True
                    elif k == 'pbk':
                        if 'reality-opts' not in self.data:
                            self.data['reality-opts'] = {}
                        self.data['reality-opts']['public-key'] = v
                    elif k == 'sid':
                        if 'reality-opts' not in self.data:
                            self.data['reality-opts'] = {}
                        self.data['reality-opts']['short-id'] = v
                    # TODO: Unused key encryption

        elif self.type == 'hysteria2':
            parsed = urlparse(url)
            self.data = {'name': unquote(parsed.fragment), 'server': parsed.hostname, 
                    'type': 'hysteria2', 'password': unquote(parsed.username)} # type: ignore
            if ':' in parsed.netloc:
                ports = parsed.netloc.split(':')[1]
                if ',' in ports:
                    self.data['port'], self.data['ports'] = ports.split(',',1)
                else:
                    self.data['port'] = ports
                try: self.data['port'] = int(self.data['port'])
                except ValueError: self.data['port'] = 443
            else:
                self.data['port'] = 443
            self.data['tls'] = False
            if parsed.query:
                k = v = ''
                for kv in parsed.query.split('&'):
                    if '=' in kv:
                        k, v = kv.split('=', 1)  # ä½¿ç”¨ maxsplit=1 æ¥å¤„ç†å€¼ä¸­åŒ…å« = çš„æƒ…å†µ
                    else:
                        v += '&' + kv
                    if k == 'insecure':
                        self.data['skip-cert-verify'] = (v != '0')
                    elif k == 'alpn':
                        self.data['alpn'] = unquote(v).split(',')
                    elif k in ('sni', 'obfs', 'obfs-password'):
                        self.data[k] = v
                    elif k == 'fp': self.data['fingerprint'] = v
        
        else: raise UnsupportedType(self.type)

    def format_name(self, max_len=30) -> None:
        import re

        self.data['name'] = self.name

        # 1. å»é™¤èŠ‚ç‚¹åç§°ä¸­çš„å¹¿å‘Š

        # 1.1 å»é™¤æ‹¬å·å†…åŒ…å«åŸŸåçš„å¹¿å‘Š
        ad_patterns = [
            r'\([^)]*\.(com|cn|net|top|xyz|org|cc|me|io|co|info|biz|vip|club|online|site|tech|store|fun|icu|link|pro|live|wang|work|to)[^)]*\)',  # è‹±æ–‡æ‹¬å·
            r'ï¼ˆ[^ï¼‰]*\.(com|cn|net|top|xyz|org|cc|me|io|co|info|biz|vip|club|online|site|tech|store|fun|icu|link|pro|live|wang|work|to)[^ï¼‰]*ï¼‰',  # ä¸­æ–‡æ‹¬å·
            r'\[[^\]]*\.(com|cn|net|top|xyz|org|cc|me|io|co|info|biz|vip|club|online|site|tech|store|fun|icu|link|pro|live|wang|work|to)[^\]]*\]',  # æ–¹æ‹¬å·
            r'ã€[^ã€‘]*\.(com|cn|net|top|xyz|org|cc|me|io|co|info|biz|vip|club|online|site|tech|store|fun|icu|link|pro|live|wang|work|to)[^ã€‘]*ã€‘',  # ä¸­æ–‡æ–¹æ‹¬å·
        ]

        for pattern in ad_patterns:
            self.data['name'] = re.sub(pattern, '', self.data['name'], flags=re.IGNORECASE)

        # 1.2 å»é™¤ç›´æ¥åŒ…å«çš„ç½‘å€ï¼ˆå¦‚ï¼šå®˜ç½‘â¶https://kelayu æˆ– å®˜ç½‘â·https://99z.toï¼‰
        # åŒ¹é… http:// æˆ– https:// å¼€å¤´çš„ç½‘å€ï¼Œä»¥åŠå‰é¢å¯èƒ½çš„æ–‡å­—
        url_patterns = [
            r'@\w+',
            r'æœºåœº',
            r'æœºåœºæ¨è',
            r'https?://[^\s]+',  # åŒ¹é…å®Œæ•´çš„URL
            r'å®˜ç½‘[â¶â·â¸â¹âºâ»â¼â½â¾â¿â“µâ“¶â“·â“¸â“¹â“ºâ“»â“¼â“½â“¾â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\d]*[^\s]*',  # åŒ¹é…"å®˜ç½‘"åŠå…¶åé¢çš„å†…å®¹
            r'[^\s]*\.(com|cn|net|top|xyz|org|cc|me|io|co|info|biz|vip|club|online|site|tech|store|fun|icu|link|pro|live|wang|work|to)/?[^\s]*',  # åŒ¹é…åŸŸå
        ]

        for pattern in url_patterns:
            self.data['name'] = re.sub(pattern, '', self.data['name'], flags=re.IGNORECASE)

        # 1.3 å»é™¤æ¶æ„æ–‡å­—å’Œä¸è‰¯å†…å®¹
        # åªåˆ é™¤æ¶æ„è¯æ±‡æœ¬èº«ï¼Œä¸åˆ é™¤æ•´ä¸ªè¯ç»„
        offensive_words = [
            r'åª.*?ä¸.*?ä¹°.*?çš„.*',  # "åª...ä¸ä¹°çš„..."å¥å¼ï¼ˆæ”¾åœ¨æœ€å‰é¢ï¼Œä¼˜å…ˆåŒ¹é…ï¼‰
            r'ç™½å«–[^\s]*',  # ç™½å«–åŠå…¶åç»­
            r'æ­».*?å®¶',  # æ­»å…¨å®¶ç­‰
            r'å‚»[é€¼æ¯”]',  # è„è¯
            r'[æ“è‰][ä½ æ³¥][å¦ˆå—é©¬]',  # è„è¯
            r'æ»šè›‹',  # ä¸ç¤¼è²Œè¯æ±‡
            r'ä¸ä¹°çš„.*',  # "ä¸ä¹°çš„..."
        ]

        for pattern in offensive_words:
            self.data['name'] = re.sub(pattern, '', self.data['name'], flags=re.IGNORECASE)

        # æ¸…ç†å¤šä½™çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦ï¼ˆåŒ…æ‹¬å¸¸è§çš„å¹¿å‘Šemojiï¼‰
        self.data['name'] = ' '.join(self.data['name'].split())
        self.data['name'] = self.data['name'].strip(' -_|ğŸ‘–ğŸğŸ‰ğŸŠğŸ’â­ğŸŒŸâœ¨')

        # 2. ä½¿ç”¨åŸæœ‰çš„ BANNED_WORDS è¿‡æ»¤
        for word in BANNED_WORDS:
            self.data['name'] = self.data['name'].replace(word, '*'*len(word))

        # 3. æ·»åŠ å“ç‰Œæ ‡è¯† uu6.top
        # ä½¿ç”¨åç¼€æ–¹å¼ï¼Œä¸å½±å“åœ°åŒºå…³é”®è¯è¯†åˆ«
        brand = "uu6.top"
        # è®¡ç®—æ·»åŠ å“ç‰Œåçš„é•¿åº¦ï¼Œç¡®ä¿ä¸è¶…è¿‡é™åˆ¶
        if self.data['name']:
            # å¦‚æœåç§°å¤ªçŸ­ï¼Œç›´æ¥æ·»åŠ ï¼›å¦‚æœå¤ªé•¿ï¼Œå…ˆæˆªæ–­å†æ·»åŠ 
            available_len = max_len - len(f" | {brand}")
            if len(self.data['name']) > available_len:
                self.data['name'] = self.data['name'][:available_len].rstrip()
            self.data['name'] = f"{self.data['name']} | {brand}"
        else:
            # å¦‚æœåç§°ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åç§°
            self.data['name'] = f"æœªå‘½å | {brand}"

        # 4. å¤„ç†é‡å
        if self.data['name'] in Node.names:
            i = 0
            new: str = self.data['name']
            while new in Node.names:
                i += 1
                new = f"{self.data['name']} #{i}"
            self.data['name'] = new
        
    @property
    def isfake(self) -> bool:
        try:
            if 'server' not in self.data: return True
            if '.' not in self.data['server']: return True
            if self.data['server'] in FAKE_IPS: return True
            if int(str(self.data['port'])) < 20: return True
            for domain in FAKE_DOMAINS:
                if self.data['server'] == domain.lstrip('.'): return True
                if self.data['server'].endswith(domain): return True
            # TODO: Fake UUID
            # if self.type == 'vmess' and len(self.data['uuid']) != len(DEFAULT_UUID):
            #     return True
            if 'sni' in self.data and 'google.com' in self.data['sni'].lower():
                # That's not designed for China
                self.data['sni'] = 'www.bing.com'
        except Exception:
            print("æ— æ³•éªŒè¯çš„èŠ‚ç‚¹ï¼", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
        return False

    @property
    def url(self) -> str:
        data = self.data
        if self.type == 'vmess':
            v = VMESS_EXAMPLE.copy()
            for key,val in data.items():
                if key in CLASH2VMESS:
                    v[CLASH2VMESS[key]] = val
            if v['net'] == 'ws':
                if 'ws-opts' in data:
                    try:
                        v['host'] = data['ws-opts']['headers']['Host']
                    except KeyError: pass
                    if 'path' in data['ws-opts']:
                        v['path'] = data['ws-opts']['path']
            elif v['net'] == 'h2':
                if 'h2-opts' in data:
                    if 'host' in data['h2-opts']:
                        v['host'] = ','.join(data['h2-opts']['host'])
                    if 'path' in data['h2-opts']:
                        v['path'] = data['h2-opts']['path']
            elif v['net'] == 'grpc':
                if 'grpc-opts' in data:
                    if 'grpc-service-name' in data['grpc-opts']:
                        v['path'] = data['grpc-opts']['grpc-service-name']
            if ('tls' in data) and data['tls']:
                v['tls'] = 'tls'
            return 'vmess://'+b64encodes(json.dumps(v, ensure_ascii=False))

        if self.type == 'ss':
            passwd = b64encodes_safe(data['cipher']+':'+data['password'])
            return f"ss://{passwd}@{data['server']}:{data['port']}#{quote(data['name'])}"
        if self.type == 'ssr':
            ret = (':'.join([str(self.data[_]) for _ in ('server','port',
                                        'protocol','cipher','obfs')]) +
                    b64encodes_safe(self.data['password']) +
                    f"remarks={b64encodes_safe(self.data['name'])}")
            for k, urlk in (('obfs-param','obfsparam'), ('protocol-param','protoparam'), ('group','group')):
                if k in self.data:
                    ret += '&'+urlk+'='+b64encodes_safe(self.data[k])
            return "ssr://"+ret

        if self.type == 'trojan':
            passwd = quote(data['password'])
            name = quote(data['name'])
            ret = f"trojan://{passwd}@{data['server']}:{data['port']}?"
            if 'skip-cert-verify' in data:
                ret += f"allowInsecure={int(data['skip-cert-verify'])}&"
            if 'sni' in data:
                ret += f"sni={data['sni']}&"
            if 'alpn' in data:
                ret += f"alpn={quote(','.join(data['alpn']))}&"
            if 'network' in data:
                if data['network'] == 'grpc':
                    service_name = data.get('grpc-opts', {}).get('grpc-service-name', '')
                    ret += f"type=grpc&serviceName={service_name}"
                elif data['network'] == 'ws':
                    ret += f"type=ws&"
                    if 'ws-opts' in data:
                        try:
                            ret += f"host={data['ws-opts']['headers']['Host']}&"
                        except KeyError: pass
                        if 'path' in data['ws-opts']:
                            ret += f"path={data['ws-opts']['path']}"
            ret = ret.rstrip('&')+'#'+name
            return ret

        if self.type == 'vless':
            passwd = quote(data['uuid'])
            name = quote(data['name'])
            ret = f"vless://{passwd}@{data['server']}:{data['port']}?"
            if 'skip-cert-verify' in data:
                ret += f"allowInsecure={int(data['skip-cert-verify'])}&"
            if 'servername' in data:
                ret += f"sni={data['servername']}&"
            if 'alpn' in data:
                ret += f"alpn={quote(','.join(data['alpn']))}&"
            if 'network' in data:
                if data['network'] == 'grpc':
                    service_name = data.get('grpc-opts', {}).get('grpc-service-name', '')
                    ret += f"type=grpc&serviceName={service_name}"
                elif data['network'] == 'ws':
                    ret += f"type=ws&"
                    if 'ws-opts' in data:
                        try:
                            ret += f"host={data['ws-opts']['headers']['Host']}&"
                        except KeyError: pass
                        if 'path' in data['ws-opts']:
                            ret += f"path={data['ws-opts']['path']}"
            if 'flow' in data:
                flow: str = data['flow']
                if flow.endswith('!'):
                    ret += f"flow={flow[:-1]}&"
                else: ret += f"flow={flow}-udp443&"
            if 'client-fingerprint' in data:
                ret += f"fp={data['client-fingerprint']}&"
            if 'tls' in data and data['tls']:
                ret += f"security=tls&"
            elif 'reality-opts' in data:
                opts: Dict[str, str] = data['reality-opts']
                ret += f"security=reality&pbk={opts.get('public-key','')}&sid={opts.get('short-id','')}&"
            ret = ret.rstrip('&')+'#'+name
            return ret

        if self.type == 'hysteria2':
            passwd = quote(data['password'])
            name = quote(data['name'])
            ret = f"hysteria2://{passwd}@{data['server']}:{data['port']}"
            if 'ports' in data:
                ret += ','+data['ports']
            ret += '?'
            if 'skip-cert-verify' in data:
                ret += f"insecure={int(data['skip-cert-verify'])}&"
            if 'alpn' in data:
                ret += f"alpn={quote(','.join(data['alpn']))}&"
            if 'fingerprint' in data:
                ret += f"fp={data['fingerprint']}&"
            for k in ('sni', 'obfs', 'obfs-password'):
                if k in data:
                    ret += f"{k}={data[k]}&"
            ret = ret.rstrip('&')+'#'+name
            return ret

        raise UnsupportedType(self.type)

    @property
    def clash_data(self) -> DATA_TYPE:
        ret = self.data.copy()
        if 'password' in ret and ret['password'].isdigit():
            ret['password'] = '!!str '+ret['password']
        if 'uuid' in ret and len(ret['uuid']) != len(DEFAULT_UUID):
            ret['uuid'] = DEFAULT_UUID
        if 'group' in ret: del ret['group']
        if 'cipher' in ret and not ret['cipher']:
            ret['cipher'] = 'auto'
        if self.type == 'vless' and 'flow' in ret:
            if ret['flow'].endswith('-udp443'):
                ret['flow'] = ret['flow'][:-7]
            elif ret['flow'].endswith('!'):
                ret['flow'] = ret['flow'][:-1]
        if 'alpn' in ret and isinstance(ret['alpn'], str):
            # 'alpn' is not a slice
            ret['alpn'] = ret['alpn'].replace(' ','').split(',')
        return ret

    def supports_meta(self, noMeta=False) -> bool:
        if self.isfake: return False
        if self.type == 'vmess':
            supported = CLASH_CIPHER_VMESS
        elif self.type == 'ss' or self.type == 'ssr':
            supported = CLASH_CIPHER_SS
        elif self.type == 'trojan': return True
        elif noMeta: return False
        else: return True
        if 'network' in self.data and self.data['network'] in ('h2','grpc'):
            # A quick fix for #2
            self.data['tls'] = True
        if 'cipher' not in self.data: return True
        if not self.data['cipher']: return True
        if self.data['cipher'] not in supported: return False
        try:
            if self.type == 'ssr':
                if 'obfs' in self.data and self.data['obfs'] not in CLASH_SSR_OBFS:
                    return False
                if 'protocol' in self.data and self.data['protocol'] not in CLASH_SSR_PROTOCOL:
                    return False
            if 'plugin-opts' in self.data and 'mode' in self.data['plugin-opts'] \
                    and not self.data['plugin-opts']['mode']: return False
        except Exception:
            print("æ— æ³•éªŒè¯çš„ Clash èŠ‚ç‚¹ï¼", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
            return False
        return True
    
    def supports_clash(self, meta=False) -> bool:
        if meta: return self.supports_meta()
        if self.type == 'vless': return False
        if self.data['type'] == 'vless': return False
        return self.supports_meta(noMeta=True)

    def supports_ray(self) -> bool:
        if self.isfake: return False
        # if self.type == 'ss':
        #     if 'plugin' in self.data and self.data['plugin']: return False
        # elif self.type == 'ssr':
        #     return False
        return True

class Source():
    @no_type_check
    def __init__(self, url: Union[str, function]) -> None:
        if isinstance(url, function):
            self.url: str = "dynamic://"+url.__name__
            self.url_source: function = url
        elif url.startswith('+'):
            self.url_source: str = url
            self.date = datetime.datetime.now()# + datetime.timedelta(days=1)
            self.gen_url()
        else:
            self.url: str = url
            self.url_source: None = None
        self.content: Union[str, List[str], int] = None
        self.sub: Union[List[str], List[Dict[str, str]]] = None
        self.cfg: Dict[str, Any] = {}

    def gen_url(self) -> None:
        self.url_source: str
        tags = self.url_source.split()
        url = tags.pop()
        while tags:
            tag = tags.pop(0)
            if tag[0] != '+': break
            if tag == '+date':
                url = self.date.strftime(url)
                self.date -= datetime.timedelta(days=1)
        self.url = url

    @no_type_check
    def get(self, depth=2) -> None:
        global exc_queue
        if self.content: return
        try:
            if self.url.startswith("dynamic:"):
                self.content: Union[str, List[str]] = self.url_source()
            else:
                global session
                if '#' in self.url:
                    segs = self.url.split('#')
                    self.cfg = dict([_.split('=',1) for _ in segs[-1].split('&')])
                    if 'max' in self.cfg:
                        try:
                            self.cfg['max'] = int(self.cfg['max'])
                        except ValueError:
                            exc_queue.append("æœ€å¤§èŠ‚ç‚¹æ•°é™åˆ¶ä¸æ˜¯æ•´æ•°ï¼")
                            del self.cfg['max']
                    if 'ignore' in self.cfg:
                        self.cfg['ignore'] = [_ for _ in self.cfg['ignore'].split(',') if _.strip()]
                    self.url = '#'.join(segs[:-1])
                with session.get(self.url, stream=True) as r:
                    if r.status_code != 200:
                        if depth > 0 and isinstance(self.url_source, str):
                            exc = f"'{self.url}' æŠ“å–æ—¶ {r.status_code}"
                            self.gen_url()
                            exc += "ï¼Œé‡æ–°ç”Ÿæˆé“¾æ¥ï¼š\n\t"+self.url
                            exc_queue.append(exc)
                            self.get(depth-1)
                        else:
                            self.content = r.status_code
                        return
                    self.content = self._download(r)
        except KeyboardInterrupt: raise
        except requests.exceptions.RequestException:
            self.content = -1
        except:
            self.content = -2
            exc = "åœ¨æŠ“å– '"+self.url+"' æ—¶å‘ç”Ÿé”™è¯¯ï¼š\n"+traceback.format_exc()
            exc_queue.append(exc)
        else:
            self.parse()

    def _download(self, r: requests.Response) -> str:
        content: str = ""
        tp = None
        pending = None
        early_stop = False
        for chunk in r.iter_content():
            if early_stop: pending = None; break
            chunk: bytes
            if pending is not None:
                chunk = pending + chunk
                pending = None
            if tp == 'sub':
                content += chunk.decode(errors='ignore')
                continue
            lines: List[bytes] = chunk.splitlines()
            if lines and lines[-1] and chunk and lines[-1][-1] == chunk[-1]:
                pending = lines.pop()
            while lines:
                line = lines.pop(0).rstrip().decode(errors='ignore').replace('\\r','')
                if not line: continue
                if not tp:
                    if ': ' in line:
                        kv = line.split(': ')
                        if len(kv) == 2 and kv[0].isalpha():
                            tp = 'yaml'
                    elif line[0] == '#': pass
                    else: tp = 'sub'
                if tp == 'yaml':
                    if content:
                        if line in ("proxy-groups:", "rules:", "script:"):
                            early_stop=True; break
                        content += line+'\n'
                    elif line == "proxies:":
                        content = line+'\n'
                elif tp == 'sub':
                    content = chunk.decode(errors='ignore')
        if pending is not None: content += pending.decode(errors='ignore')
        return content

    def parse(self) -> None:
        global exc_queue
        try:
            text = self.content
            if isinstance(text, str):
                if "proxies:" in text:
                    # Clash config
                    config = yaml.full_load(text.replace("!<str>","!!str"))
                    sub = config['proxies']
                elif '://' in text:
                    # V2Ray raw list
                    sub = text.strip().splitlines()
                else:
                    # V2Ray Sub
                    try:
                        sub = b64decodes(text.strip()).strip().splitlines()
                    except (UnicodeDecodeError, binascii.Error) as e:
                        exc_queue.append(f"base64 è§£ç å¤±è´¥: {type(e).__name__}")
                        self.sub = []
                        return
            else: sub = text # åŠ¨æ€èŠ‚ç‚¹æŠ“å–åç›´æ¥ä¼ å…¥åˆ—è¡¨

            if 'max' in self.cfg and len(sub) > self.cfg['max']:
                exc_queue.append(f"æ­¤è®¢é˜…æœ‰ {len(sub)} ä¸ªèŠ‚ç‚¹ï¼Œæœ€å¤§é™åˆ¶ä¸º {self.cfg['max']} ä¸ªï¼Œå¿½ç•¥æ­¤è®¢é˜…ã€‚")
                self.sub = []
            elif sub and 'ignore' in self.cfg:
                if isinstance(sub[0], str):
                    self.sub = [_ for _ in sub if _.split('://', 1)[0] not in self.cfg['ignore']]
                elif isinstance(sub[0], dict):
                    self.sub = [_ for _ in sub if _.get('type', '') not in self.cfg['ignore']] #type:ignore
                else: self.sub = sub
            else: self.sub = sub
        except KeyboardInterrupt: raise
        except: exc_queue.append(
                "åœ¨è§£æ '"+self.url+"' æ—¶å‘ç”Ÿé”™è¯¯ï¼š\n"+traceback.format_exc())

class DomainTree:
    def __init__(self) -> None:
        self.children: Dict[str, __class__] = {}
        self.here: bool = False

    def insert(self, domain: str) -> None:
        segs = domain.split('.')
        segs.reverse()
        self._insert(segs)

    def _insert(self, segs: List[str]) -> None:
        if not segs:
            self.here = True
            return
        if segs[0] not in self.children:
            self.children[segs[0]] = __class__()
        child = self.children[segs[0]]
        del segs[0]
        child._insert(segs)

    def remove(self, domain: str) -> None:
        segs = domain.split('.')
        segs.reverse()
        self._remove(segs)

    def _remove(self, segs: List[str]) -> None:
        self.here = False
        if not segs:
            self.children.clear()
            return
        if segs[0] in self.children:
            child = self.children[segs[0]]
            del segs[0]
            child._remove(segs)

    def get(self) -> List[str]:
        ret: List[str] = []
        for name, child in self.children.items():
            if child.here: ret.append(name)
            else: ret.extend([_+'.'+name for _ in child.get()])
        return ret

def extract(url: str) -> Union[Set[str], int]:
    global session
    res = session.get(url)
    if res.status_code != 200: return res.status_code
    urls: Set[str] = set()
    if '#' in url:
        mark = '#'+url.split('#', 1)[1]
    else:
        mark = ''
    for line in res.text.strip().splitlines():
        if line.startswith("http"):
            urls.add(line+mark)
    return urls

merged: Dict[int, Node] = {}
unknown: Set[str] = set()
used: Dict[int, Dict[int, str]] = {}
def merge(source_obj: Source, sourceId=-1) -> None:
    global merged, unknown
    sub = source_obj.sub
    if not sub: print("ç©ºè®¢é˜…ï¼Œè·³è¿‡ï¼", end='', flush=True); return
    for p in sub:
        if isinstance(p, str) and '://' not in p: continue
        try: n = Node(p)
        except KeyboardInterrupt: raise
        except UnsupportedType as e:
            if len(e.args) == 1:
                print(f"ä¸æ”¯æŒçš„ç±»å‹ï¼š{e}")
            unknown.add(p) # type: ignore
        except Exception as e:
            # æ‰“å°é”™è¯¯ç±»å‹å’Œç®€çŸ­ä¿¡æ¯
            error_type = type(e).__name__
            error_msg = str(e)
            # æ‰“å°è§£æå¤±è´¥çš„æ•°æ®ï¼ˆæˆªå–å‰100ä¸ªå­—ç¬¦ï¼‰
            data_preview = str(p)[:100] if isinstance(p, str) else str(p)[:100]
            print(f"è§£æèŠ‚ç‚¹å¤±è´¥ ({error_type}: {error_msg}) - æ•°æ®: {data_preview}", flush=True)
        else:
            n.format_name()
            Node.names.add(n.data['name'])
            hashn = hash(n)
            if hashn not in merged:
                merged[hashn] = n
            else:
                merged[hashn].data.update(n.data)
            if hashn not in used:
                used[hashn] = {}
            used[hashn][sourceId] = n.name

def raw2fastly(url: str) -> str:
    if not LOCAL: return url
    url: Union[str, List[str]]
    if url.startswith("https://raw.githubusercontent.com/"):
        # url = url[34:].split('/')
        # url[1] += '@'+url[2]
        # del url[2]
        # url = "https://fastly.jsdelivr.net/gh/"+('/'.join(url))
        # return url
        return "https://ghproxy.cn/"+url
    return url

def test_node_delay(node: Node, timeout: float = 1.0) -> Optional[float]:
    """
    æµ‹è¯•èŠ‚ç‚¹çš„TCPè¿æ¥å»¶è¿Ÿ
    è¿”å›å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        server = node.data.get('server')
        port = node.data.get('port')

        if not server or not port:
            return None

        # å°è¯•å°†ç«¯å£è½¬æ¢ä¸ºæ•´æ•°
        try:
            port = int(port)
        except (ValueError, TypeError):
            return None

        # æµ‹è¯•TCPè¿æ¥
        start_time = time.time()
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(timeout)

        try:
            sock.connect((server, port))
            delay = time.time() - start_time
            sock.close()
            return delay
        except (socket.timeout, socket.error, OSError):
            return None
        finally:
            try:
                sock.close()
            except:
                pass
    except Exception:
        return None

def find_clash_executable() -> Optional[str]:
    """
    æŸ¥æ‰¾ç³»ç»Ÿä¸­çš„Clashå¯æ‰§è¡Œæ–‡ä»¶
    ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡CLASH_BINARYæŒ‡å®šçš„è·¯å¾„
    """
    # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„è·¯å¾„
    clash_binary = os.environ.get('CLASH_BINARY')
    if clash_binary and os.path.isfile(clash_binary) and os.access(clash_binary, os.X_OK):
        return clash_binary

    # åœ¨PATHä¸­æŸ¥æ‰¾
    possible_names = ['mihomo', 'clash-meta', 'clash.meta', 'clash']

    for name in possible_names:
        path = shutil.which(name)
        if path:
            return path

    return None

def test_nodes_with_clash(nodes_dict: Dict[int, Node], max_delay: int = 1000, test_urls: Optional[List[str]] = None, max_retries: int = 1, concurrent_tests: int = 15) -> Dict[int, Node]:
    """
    ä½¿ç”¨Clash APIæµ‹è¯•èŠ‚ç‚¹å»¶è¿Ÿ
    è¿™æ˜¯æœ€å‡†ç¡®çš„æµ‹è¯•æ–¹æ³•ï¼Œä¼šå®é™…é€šè¿‡ä»£ç†å‘é€è¯·æ±‚

    å‚æ•°:
        nodes_dict: èŠ‚ç‚¹å­—å…¸
        max_delay: æœ€å¤§å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
        test_urls: æµ‹è¯•URLåˆ—è¡¨ï¼Œä¼šä¾æ¬¡å°è¯•ç›´åˆ°æˆåŠŸ
        max_retries: æ¯ä¸ªURLçš„æœ€å¤§é‡è¯•æ¬¡æ•°
        concurrent_tests: å¹¶å‘æµ‹è¯•æ•°é‡
    """
    # æ£€æµ‹æ˜¯å¦åœ¨GitHub Actionsç¯å¢ƒ
    is_github_actions = os.environ.get('GITHUB_ACTIONS') == 'true'

    # é»˜è®¤ä½¿ç”¨å¤šä¸ªæµ‹è¯•URLï¼Œæé«˜æµ‹è¯•æˆåŠŸç‡
    if test_urls is None:
        if is_github_actions:
            # GitHub Actionsç¯å¢ƒï¼šä½¿ç”¨å›½å¤–URL
            test_urls = [
                "http://www.gstatic.com/generate_204",
                "http://cp.cloudflare.com/generate_204"
            ]
        else:
            # æœ¬åœ°ç¯å¢ƒï¼šä¼˜å…ˆä½¿ç”¨å›½å†…å¤–éƒ½å¯è®¿é—®çš„URL
            test_urls = [
                "http://cp.cloudflare.com/generate_204",
                "http://www.gstatic.com/generate_204",
                "http://captive.apple.com/hotspot-detect.html"
            ]

    clash_bin = find_clash_executable()
    if not clash_bin:
        print("=" * 60)
        print("è­¦å‘Šï¼šæœªæ‰¾åˆ°Clashå¯æ‰§è¡Œæ–‡ä»¶")
        print("å°†ä½¿ç”¨TCPè¿æ¥æµ‹è¯•ï¼ˆä¸å¦‚Clash APIæµ‹è¯•å‡†ç¡®ï¼‰")
        print("æç¤ºï¼šåœ¨GitHub Actionsä¸­ä¼šè‡ªåŠ¨ä¸‹è½½mihomoè¿›è¡Œæµ‹è¯•")
        print("=" * 60)
        return filter_nodes_by_delay_tcp(nodes_dict, max_delay=max_delay/1000.0)

    print(f"ä½¿ç”¨ {os.path.basename(clash_bin)} æµ‹è¯•èŠ‚ç‚¹å»¶è¿Ÿï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")

    # åˆ›å»ºä¸´æ—¶ç›®å½•å’Œé…ç½®æ–‡ä»¶
    temp_dir = tempfile.mkdtemp(prefix='clash_test_')
    config_path = os.path.join(temp_dir, 'config.yaml')

    try:
        # ç”ŸæˆClashé…ç½®ï¼Œè¿‡æ»¤æ‰æœ‰é—®é¢˜çš„èŠ‚ç‚¹
        proxies = []
        node_names = {}
        skipped = 0
        skipped_reasons = {}
        for hash_id, node in nodes_dict.items():
            if node.supports_meta():
                proxy_data = node.clash_data
                skip_reason = None
                
                # éªŒè¯ REALITY é…ç½®
                if 'reality-opts' in proxy_data:
                    opts = proxy_data.get('reality-opts', {})
                    short_id = opts.get('short-id', '')
                    public_key = opts.get('public-key', '')
                    
                    # public-key æ˜¯å¿…é¡»çš„
                    if not public_key:
                        skip_reason = "REALITYç¼ºå°‘public-key"
                    # short-id å¿…é¡»æ˜¯æœ‰æ•ˆçš„åå…­è¿›åˆ¶ï¼Œä¸”é•¿åº¦å¿…é¡»æ˜¯ 0, 8, æˆ– 16ï¼ˆmihomoè¦æ±‚ï¼‰
                    elif short_id:
                        valid_lengths = [0, 8, 16]
                        try:
                            bytes.fromhex(short_id)
                            if len(short_id) not in valid_lengths:
                                skip_reason = f"REALITY short-idé•¿åº¦æ— æ•ˆ({len(short_id)})"
                        except ValueError:
                            skip_reason = f"REALITY short-idæ ¼å¼æ— æ•ˆ"
                
                if skip_reason:
                    skipped += 1
                    skipped_reasons[skip_reason] = skipped_reasons.get(skip_reason, 0) + 1
                    continue
                    
                proxies.append(proxy_data)
                node_names[node.data['name']] = (hash_id, node)
        
        if skipped > 0:
            print(f"è·³è¿‡ {skipped} ä¸ªé…ç½®æ— æ•ˆçš„èŠ‚ç‚¹:")
            for reason, count in skipped_reasons.items():
                print(f"  - {reason}: {count}ä¸ª")

        # å®Œå–„çš„Clashé…ç½®ï¼ŒåŒ…å«DNSè®¾ç½®
        config = {
            'port': 17890,
            'socks-port': 17891,
            'allow-lan': False,
            'mode': 'global',
            'log-level': 'silent',
            'external-controller': '127.0.0.1:19090',
            'dns': {
                'enable': True,
                'listen': '0.0.0.0:1053',
                'enhanced-mode': 'fake-ip',
                'nameserver': [
                    '223.5.5.5',
                    '119.29.29.29',
                    '8.8.8.8',
                    '1.1.1.1'
                ],
                'fallback': [
                    'https://1.1.1.1/dns-query',
                    'https://dns.google/dns-query'
                ]
            },
            'proxies': proxies
        }

        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, allow_unicode=True)

        # å…ˆæµ‹è¯•é…ç½®æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        print(f"æµ‹è¯•é…ç½®æ–‡ä»¶ï¼ˆå…± {len(proxies)} ä¸ªèŠ‚ç‚¹ï¼‰...")
        # è¾“å‡ºå‰10ä¸ªèŠ‚ç‚¹çš„ä¿¡æ¯ç”¨äºè°ƒè¯•
        for i, p in enumerate(proxies[:10]):
            has_reality = 'reality-opts' in p
            reality_info = ""
            if has_reality:
                opts = p.get('reality-opts', {})
                sid = opts.get('short-id', '<æ— >')
                pbk = opts.get('public-key', '<æ— >')[:20] if opts.get('public-key') else '<æ— >'
                reality_info = f" sid={sid} pbk={pbk}..."
            print(f"  èŠ‚ç‚¹{i}: {p.get('name', 'unknown')[:30]} type={p.get('type')} reality={has_reality}{reality_info}")
        test_result = subprocess.run(
            [clash_bin, '-t', '-f', config_path],
            capture_output=True,
            text=True,
            timeout=30
        )
        if test_result.returncode != 0:
            print(f"é…ç½®æ–‡ä»¶æµ‹è¯•å¤±è´¥ï¼Œé€€å‡ºç : {test_result.returncode}")
            print(f"é”™è¯¯è¾“å‡º: {test_result.stderr[:2000] if test_result.stderr else 'æ— '}")
            print(f"æ ‡å‡†è¾“å‡º: {test_result.stdout[:2000] if test_result.stdout else 'æ— '}")
            print("=" * 60)
            print("å°†è¿”å› Noneï¼Œä½¿ç”¨ä¸Šæ¬¡çš„èŠ‚ç‚¹")
            print("=" * 60)
            return None

        # å¯åŠ¨Clashè¿›ç¨‹
        print(f"å¯åŠ¨Clashè¿›ç¨‹...")
        process = subprocess.Popen(
            [clash_bin, '-d', temp_dir, '-f', config_path],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )

        # ç­‰å¾…Clashå¯åŠ¨å¹¶æ£€æŸ¥APIæ˜¯å¦å¯ç”¨
        api_base = 'http://127.0.0.1:19090'
        startup_retries = 30  # å¢åŠ ç­‰å¾…æ—¶é—´
        clash_started = False
        for i in range(startup_retries):
            # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å·²é€€å‡º
            if process.poll() is not None:
                stderr_output = process.stderr.read().decode('utf-8', errors='ignore')
                print(f"é”™è¯¯ï¼šClashè¿›ç¨‹æ„å¤–é€€å‡ºï¼Œé€€å‡ºç : {process.returncode}")
                if stderr_output:
                    print(f"é”™è¯¯ä¿¡æ¯: {stderr_output[:500]}")
                break
            try:
                response = requests.get(f"{api_base}/version", timeout=2)
                if response.status_code == 200:
                    print(f"Clashå·²å¯åŠ¨ï¼Œç‰ˆæœ¬: {response.json().get('version', 'unknown')}")
                    clash_started = True
                    break
            except:
                pass
            time.sleep(1)  # å¢åŠ æ¯æ¬¡ç­‰å¾…æ—¶é—´
        
        if not clash_started:
            print("=" * 60)
            print("é”™è¯¯ï¼šClashå¯åŠ¨å¤±è´¥ï¼Œæ— æ³•è¿›è¡ŒèŠ‚ç‚¹æµ‹è¯•")
            # å°è¯•è¯»å–é”™è¯¯è¾“å‡º
            try:
                stderr_output = process.stderr.read().decode('utf-8', errors='ignore')
                if stderr_output:
                    print(f"Clashé”™è¯¯è¾“å‡º: {stderr_output[:1000]}")
            except:
                pass
            print("æç¤ºï¼šè¿™å¯èƒ½æ˜¯ GitHub Actions ç¯å¢ƒé™åˆ¶å¯¼è‡´çš„")
            print("å°†è¿”å› Noneï¼Œä½¿ç”¨ä¸Šæ¬¡çš„èŠ‚ç‚¹")
            print("=" * 60)
            return None

        # æµ‹è¯•èŠ‚ç‚¹ï¼ˆä½¿ç”¨å¹¶å‘ï¼‰
        valid_nodes: Dict[int, Node] = {}
        total = len(node_names)
        tested = 0
        valid = 0
        error_stats: Dict[str, int] = {}
        test_lock = threading.Lock()

        print(f"å¼€å§‹æµ‹è¯• {total} ä¸ªèŠ‚ç‚¹çš„å»¶è¿Ÿ")
        print(f"  - è¶…æ—¶æ—¶é—´: {max_delay}ms")
        print(f"  - æµ‹è¯•URL: {len(test_urls)}ä¸ªå¤‡é€‰")
        print(f"  - é‡è¯•æ¬¡æ•°: {max_retries}æ¬¡")
        print(f"  - å¹¶å‘æ•°: {concurrent_tests}")
        print(f"  - ç¯å¢ƒ: {'GitHub Actions' if is_github_actions else 'æœ¬åœ°'}")
        print("-" * 60)

        def test_single_node(name: str, hash_id: int, node: Node) -> Tuple[bool, Optional[int], Optional[str]]:
            """æµ‹è¯•å•ä¸ªèŠ‚ç‚¹ï¼Œè¿”å›(æ˜¯å¦æœ‰æ•ˆ, å»¶è¿Ÿ, é”™è¯¯ä¿¡æ¯)"""
            from urllib.parse import quote as url_quote
            encoded_name = url_quote(name)

            # å°è¯•å¤šä¸ªæµ‹è¯•URL
            for test_url in test_urls:
                # å¯¹æ¯ä¸ªURLè¿›è¡Œé‡è¯•
                for retry in range(max_retries):
                    try:
                        url = f"{api_base}/proxies/{encoded_name}/delay?timeout={max_delay}&url={test_url}"
                        response = requests.get(url, timeout=max_delay/1000.0 + 10)

                        if response.status_code == 200:
                            data = response.json()
                            delay = data.get('delay', 0)
                            if delay > 0 and delay <= max_delay:
                                return True, delay, None
                            else:
                                last_error = f"å»¶è¿Ÿè¿‡é«˜({delay}ms)"
                        else:
                            try:
                                error_data = response.json()
                                last_error = error_data.get('message', f'HTTP {response.status_code}')
                            except:
                                last_error = f'HTTP {response.status_code}'

                    except requests.exceptions.Timeout:
                        last_error = "Timeout"
                    except requests.exceptions.ConnectionError:
                        last_error = "è¿æ¥é”™è¯¯"
                    except Exception as e:
                        last_error = str(e)[:50]

                    # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡é‡è¯•ï¼Œç¨å¾®ç­‰å¾…ä¸€ä¸‹
                    if retry < max_retries - 1:
                        time.sleep(0.2)

            return False, None, last_error

        def process_node(item):
            """å¤„ç†å•ä¸ªèŠ‚ç‚¹çš„æµ‹è¯•"""
            nonlocal tested, valid
            name, (hash_id, node) = item

            is_valid, delay, error = test_single_node(name, hash_id, node)

            with test_lock:
                tested += 1
                if is_valid:
                    valid += 1
                    valid_nodes[hash_id] = node
                    print(f"[{tested}/{total}] âœ“ {name[:40]} - {delay}ms", flush=True)
                else:
                    error_key = error if error else "æœªçŸ¥é”™è¯¯"
                    error_stats[error_key] = error_stats.get(error_key, 0) + 1
                    print(f"[{tested}/{total}] âœ— {name[:40]} - {error}", flush=True)

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æµ‹è¯•
        with ThreadPoolExecutor(max_workers=concurrent_tests) as executor:
            executor.map(process_node, node_names.items())

        print("-" * 60)
        print(f"Clashå»¶è¿Ÿæµ‹è¯•å®Œæˆï¼æœ‰æ•ˆèŠ‚ç‚¹: {valid}/{total} ({valid*100//total if total > 0 else 0}%)")

        # è¾“å‡ºé”™è¯¯ç»Ÿè®¡
        if error_stats:
            print("\né”™è¯¯ç»Ÿè®¡:")
            for error, count in sorted(error_stats.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"  - {error}: {count}æ¬¡")

        return valid_nodes

    finally:
        # æ¸…ç†
        try:
            process.terminate()
            process.wait(timeout=5)
        except:
            try:
                process.kill()
            except:
                pass

        try:
            shutil.rmtree(temp_dir)
        except:
            pass

def filter_nodes_by_delay_tcp(nodes_dict: Dict[int, Node], max_delay: float = 1.0, max_workers: int = 50) -> Dict[int, Node]:
    """
    ä½¿ç”¨TCPè¿æ¥æµ‹è¯•èŠ‚ç‚¹å»¶è¿Ÿï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
    """
    valid_nodes: Dict[int, Node] = {}
    total = len(nodes_dict)
    tested = 0
    valid = 0

    print(f"å¼€å§‹æµ‹è¯• {total} ä¸ªèŠ‚ç‚¹çš„TCPè¿é€šæ€§ï¼ˆè¶…æ—¶æ—¶é—´: {max_delay}ç§’ï¼‰...")

    def test_single_node(item: Tuple[int, Node]) -> Tuple[int, Node, Optional[float]]:
        hash_id, node = item
        delay = test_node_delay(node, timeout=max_delay)
        return hash_id, node, delay

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(test_single_node, item): item for item in nodes_dict.items()}

        for future in as_completed(futures):
            tested += 1
            try:
                hash_id, node, delay = future.result()

                if delay is not None:
                    valid += 1
                    valid_nodes[hash_id] = node
                    print(f"[{tested}/{total}] âœ“ {node.data['name'][:30]} - {int(delay*1000)}ms", flush=True)
                else:
                    print(f"[{tested}/{total}] âœ— {node.data['name'][:30]} - è¿æ¥å¤±è´¥", flush=True)
            except Exception as e:
                print(f"[{tested}/{total}] âœ— æµ‹è¯•å‡ºé”™: {e}", flush=True)

    print(f"\nTCPè¿é€šæ€§æµ‹è¯•å®Œæˆï¼æœ‰æ•ˆèŠ‚ç‚¹: {valid}/{total}")
    return valid_nodes

def filter_nodes_by_delay(nodes_dict: Dict[int, Node], max_delay: float = 1.0, max_workers: int = 50, use_clash: bool = True, test_urls: Optional[List[str]] = None, concurrent_tests: int = 15) -> Dict[int, Node]:
    """
    æµ‹è¯•èŠ‚ç‚¹å»¶è¿Ÿå¹¶è¿‡æ»¤

    å‚æ•°:
        nodes_dict: èŠ‚ç‚¹å­—å…¸
        max_delay: æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œå¯¹äºClashæµ‹è¯•ä¼šè½¬æ¢ä¸ºæ¯«ç§’
        max_workers: TCPæµ‹è¯•çš„å¹¶å‘æ•°
        use_clash: True=ä½¿ç”¨Clash APIæµ‹è¯•ï¼ˆæ¨èï¼‰ï¼ŒFalse=ä½¿ç”¨TCPè¿æ¥æµ‹è¯•
        test_urls: Clashæµ‹è¯•ä½¿ç”¨çš„URLåˆ—è¡¨
        concurrent_tests: Clashæµ‹è¯•çš„å¹¶å‘æ•°
    """
    if use_clash:
        return test_nodes_with_clash(nodes_dict, max_delay=int(max_delay*1000), test_urls=test_urls, concurrent_tests=concurrent_tests)
    else:
        return filter_nodes_by_delay_tcp(nodes_dict, max_delay=max_delay, max_workers=max_workers)

def merge_adblock(adblock_name: str, rules: Dict[str, str]) -> None:
    print("æ­£åœ¨è§£æ Adblock åˆ—è¡¨... ", end='', flush=True)
    blocked: Set[str] = set()
    unblock: Set[str] = set()
    for url in ABFURLS:
        url = raw2fastly(url)
        try:
            res = session.get(url)
        except requests.exceptions.RequestException as e:
            try:
                print(f"{url} ä¸‹è½½å¤±è´¥ï¼š{e.args[0].reason}")
            except Exception:
                print(f"{url} ä¸‹è½½å¤±è´¥ï¼šæ— æ³•è§£æçš„é”™è¯¯ï¼")
                traceback.print_exc()
            continue
        if res.status_code != 200:
            print(url, res.status_code)
            continue
        for line in res.text.strip().splitlines():
            line = line.strip()
            if not line or line[0] in '!#': continue
            elif line[:2] == '@@':
                unblock.add(line.split('^')[0].strip('@|^'))
            elif line[:2] == '||' and ('/' not in line) and ('?' not in line) and \
                            (line[-1] == '^' or line.endswith("$all")):
                blocked.add(line.strip('al').strip('|^$'))

    for url in ABFWHITE:
        url = raw2fastly(url)
        try:
            res = session.get(url)
        except requests.exceptions.RequestException as e:
            try:
                print(f"{url} ä¸‹è½½å¤±è´¥ï¼š{e.args[0].reason}")
            except Exception:
                print(f"{url} ä¸‹è½½å¤±è´¥ï¼šæ— æ³•è§£æçš„é”™è¯¯ï¼")
                traceback.print_exc()
            continue
        if res.status_code != 200:
            print(url, res.status_code)
            continue
        for line in res.text.strip().splitlines():
            line = line.strip()
            if not line or line[0] == '!': continue
            else: unblock.add(line.split('^')[0].strip('|^'))

    domain_root = DomainTree()
    domain_keys: Set[str] = set()
    for domain in blocked:
        if '/' in domain: continue
        if '*' in domain:
            domain = domain.strip('*')
            if '*' not in domain:
                domain_keys.add(domain)
            continue
        segs = domain.split('.')
        if len(segs) == 4 and domain.replace('.','').isdigit(): # IP
            for seg in segs: # '223.73.212.020' is not valid
                if not seg: break
                if seg[0] == '0' and seg != '0': break
            else:
                rules[f'IP-CIDR,{domain}/32'] = adblock_name
        else:
            domain_root.insert(domain)
    for domain in unblock:
        domain_root.remove(domain)

    for domain in domain_keys:
        rules[f'DOMAIN-KEYWORD,{domain}'] = adblock_name

    for domain in domain_root.get():
        for key in domain_keys:
            if key in domain: break
        else: rules[f'DOMAIN-SUFFIX,{domain}'] = adblock_name

    print(f"å…±æœ‰ {len(rules)} æ¡è§„åˆ™")

def load_previous_nodes() -> List[str]:
    """
    ä»ä¹‹å‰ç”Ÿæˆçš„ç»“æœæ–‡ä»¶ä¸­åŠ è½½èŠ‚ç‚¹
    è¿”å›èŠ‚ç‚¹URLåˆ—è¡¨
    """
    previous_nodes: List[str] = []

    # å°è¯•ä» list.meta.yml è¯»å–èŠ‚ç‚¹
    try:
        print("æ­£åœ¨è¯»å–ä¹‹å‰çš„èŠ‚ç‚¹ç»“æœ (list.meta.yml)... ", end='', flush=True)
        with open("list.meta.yml", encoding="utf-8") as f:
            content = f.read()
            # è·³è¿‡ç¬¬ä¸€è¡Œçš„æ—¶é—´æˆ³æ³¨é‡Š
            if content.startswith('#'):
                content = '\n'.join(content.split('\n')[1:])
            config = yaml.full_load(content)
            if config and 'proxies' in config:
                proxies = config['proxies']
                print(f"æ‰¾åˆ° {len(proxies)} ä¸ªèŠ‚ç‚¹")
                # å°† Clash æ ¼å¼çš„èŠ‚ç‚¹è½¬æ¢å› Node å¯¹è±¡
                for proxy in proxies:
                    try:
                        node = Node(proxy)
                        previous_nodes.append(node.url)
                    except Exception as e:
                        # å¿½ç•¥æ— æ³•è½¬æ¢çš„èŠ‚ç‚¹
                        pass
                print(f"æˆåŠŸåŠ è½½ {len(previous_nodes)} ä¸ªä¹‹å‰çš„èŠ‚ç‚¹")
            else:
                print("æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
    except FileNotFoundError:
        print("æœªæ‰¾åˆ°ä¹‹å‰çš„ç»“æœæ–‡ä»¶")
    except Exception as e:
        print(f"è¯»å–å¤±è´¥: {e}")
        traceback.print_exc()

    return previous_nodes

# ============================================================
# æºå†å²è®°å½•ç®¡ç†åŠŸèƒ½
# ============================================================

def load_source_history() -> Dict[str, List[Dict[str, Any]]]:
    """
    åŠ è½½æºå†å²è®°å½•
    è¿”å›æ ¼å¼: {url: [{date: "YYYY-MM-DD", success: bool, valid_nodes: int}, ...]}
    """
    if os.path.exists(SOURCE_HISTORY_FILE):
        try:
            with open(SOURCE_HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError) as e:
            print(f"åŠ è½½æºå†å²è®°å½•å¤±è´¥: {e}")
    return {}

def save_source_history(history: Dict[str, List[Dict[str, Any]]]) -> None:
    """ä¿å­˜æºå†å²è®°å½•"""
    try:
        with open(SOURCE_HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    except IOError as e:
        print(f"ä¿å­˜æºå†å²è®°å½•å¤±è´¥: {e}")

def update_source_history(history: Dict[str, List[Dict[str, Any]]], 
                          url: str, success: bool, valid_nodes: int) -> None:
    """
    æ›´æ–°å•ä¸ªæºçš„å†å²è®°å½•
    åªä¿ç•™æœ€è¿‘7å¤©çš„è®°å½•
    """
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    
    if url not in history:
        history[url] = []
    
    # æ£€æŸ¥ä»Šå¤©æ˜¯å¦å·²æœ‰è®°å½•ï¼Œå¦‚æœæœ‰åˆ™æ›´æ–°
    for record in history[url]:
        if record['date'] == today:
            # æ›´æ–°ä»Šå¤©çš„è®°å½•ï¼ˆå–æ›´å¥½çš„ç»“æœï¼‰
            if success and valid_nodes > record['valid_nodes']:
                record['success'] = success
                record['valid_nodes'] = valid_nodes
            return
    
    # æ·»åŠ ä»Šå¤©çš„è®°å½•
    history[url].append({
        'date': today,
        'success': success,
        'valid_nodes': valid_nodes
    })
    
    # åªä¿ç•™æœ€è¿‘7å¤©çš„è®°å½•
    cutoff_date = (datetime.datetime.now() - datetime.timedelta(days=INVALID_DAYS_THRESHOLD)).strftime("%Y-%m-%d")
    history[url] = [r for r in history[url] if r['date'] >= cutoff_date]

def normalize_source_url(line: str) -> Optional[str]:
    """
    ä» sources.list çš„è¡Œä¸­æå–è§„èŒƒåŒ–çš„ URL
    è¿”å› None è¡¨ç¤ºè¿™æ˜¯æ³¨é‡Šè¡Œæˆ–ç©ºè¡Œ
    """
    line = line.strip()
    if not line or line.startswith('#'):
        return None
    
    # å»æ‰å‰ç¼€æ ‡è®°
    url = line
    if url.startswith('!'):
        url = url[1:]
    if url.startswith('*'):
        url = url[1:]
    if url.startswith('+'):
        # åŠ¨æ€æ—¥æœŸURLï¼Œå–æœ€åä¸€éƒ¨åˆ†
        parts = url.split()
        url = parts[-1] if parts else url
    
    # å»æ‰URLå‚æ•°éƒ¨åˆ†ï¼ˆ#åé¢çš„ï¼‰
    if '#' in url:
        url = url.split('#')[0]
    
    return url

def check_source_should_delete(history: Dict[str, List[Dict[str, Any]]], url: str) -> Tuple[bool, str]:
    """
    æ£€æŸ¥æºæ˜¯å¦åº”è¯¥è¢«åˆ é™¤
    è¿”å›: (æ˜¯å¦åº”åˆ é™¤, åˆ é™¤åŸå› )
    è§„åˆ™:
    1. 7å¤©å†…æ‰€æœ‰è®¿é—®éƒ½å¤±è´¥
    2. 7å¤©å†…æ‰€æœ‰è·å–çš„ä»£ç†éƒ½æ— æ•ˆï¼ˆvalid_nodes=0ï¼‰
    """
    if url not in history:
        return False, ""
    
    records = history[url]
    
    # å¿…é¡»æœ‰è¶³å¤Ÿçš„è®°å½•ï¼ˆè‡³å°‘7å¤©çš„æ•°æ®ï¼‰
    if len(records) < INVALID_DAYS_THRESHOLD:
        return False, ""
    
    # æ£€æŸ¥æœ€è¿‘7å¤©çš„è®°å½•
    all_failed = all(not r['success'] for r in records)
    all_no_valid_nodes = all(r['valid_nodes'] == 0 for r in records)
    
    if all_failed:
        return True, f"è¿ç»­{len(records)}å¤©è®¿é—®å¤±è´¥"
    if all_no_valid_nodes:
        return True, f"è¿ç»­{len(records)}å¤©æ— æœ‰æ•ˆä»£ç†"
    
    return False, ""

def cleanup_invalid_sources(history: Dict[str, List[Dict[str, Any]]]) -> List[Tuple[str, str, str]]:
    """
    æ¸…ç†æ— æ•ˆçš„è®¢é˜…æº
    è¿”å›è¢«åˆ é™¤çš„æºåˆ—è¡¨: [(åŸå§‹è¡Œ, è§„èŒƒåŒ–URL, åˆ é™¤åŸå› ), ...]
    """
    deleted_sources: List[Tuple[str, str, str]] = []
    
    # è¯»å– sources.list
    try:
        with open(SOURCES_FILE, 'r', encoding='utf-8') as f:
            lines = f.readlines()
    except IOError as e:
        print(f"è¯»å– {SOURCES_FILE} å¤±è´¥: {e}")
        return deleted_sources
    
    # æ£€æŸ¥æ¯ä¸€è¡Œ
    new_lines: List[str] = []
    for line in lines:
        original_line = line.rstrip('\n')
        url = normalize_source_url(original_line)
        
        if url is None:
            # ä¿ç•™æ³¨é‡Šå’Œç©ºè¡Œ
            new_lines.append(line)
            continue
        
        should_delete, reason = check_source_should_delete(history, url)
        if should_delete:
            deleted_sources.append((original_line, url, reason))
            # ä¸æ·»åŠ åˆ° new_linesï¼Œç›¸å½“äºåˆ é™¤
        else:
            new_lines.append(line)
    
    if deleted_sources:
        # å†™å› sources.list
        try:
            with open(SOURCES_FILE, 'w', encoding='utf-8') as f:
                f.writelines(new_lines)
            print(f"å·²ä» {SOURCES_FILE} åˆ é™¤ {len(deleted_sources)} ä¸ªæ— æ•ˆæº")
        except IOError as e:
            print(f"å†™å…¥ {SOURCES_FILE} å¤±è´¥: {e}")
            return []
        
        # è®°å½•åˆ° source_delete.list
        try:
            with open(SOURCE_DELETE_FILE, 'a', encoding='utf-8') as f:
                today = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
                for original_line, url, reason in deleted_sources:
                    f.write(f"# [{today}] {reason}\n")
                    f.write(f"{original_line}\n")
            print(f"å·²å°†åˆ é™¤è®°å½•è¿½åŠ åˆ° {SOURCE_DELETE_FILE}")
        except IOError as e:
            print(f"å†™å…¥ {SOURCE_DELETE_FILE} å¤±è´¥: {e}")
        
        # ä»å†å²è®°å½•ä¸­åˆ é™¤è¿™äº›æº
        for _, url, _ in deleted_sources:
            if url in history:
                del history[url]
        save_source_history(history)
    
    return deleted_sources

def main():
    global exc_queue, merged, FETCH_TIMEOUT, ABFURLS, AUTOURLS, AUTOFETCH
    
    # åŠ è½½æºå†å²è®°å½•
    source_history = load_source_history()
    print(f"å·²åŠ è½½ {len(source_history)} ä¸ªæºçš„å†å²è®°å½•")
    
    sources = open("sources.list", encoding="utf-8").read().strip().splitlines()
    if DEBUG_NO_NODES:
        # !!! JUST FOR DEBUGING !!!
        print("!!! è­¦å‘Šï¼šæ‚¨å·²å¯ç”¨æ— èŠ‚ç‚¹è°ƒè¯•ï¼Œç¨‹åºäº§ç”Ÿçš„é…ç½®ä¸èƒ½è¢«ç›´æ¥ä½¿ç”¨ !!!")
        sources = []
    if DEBUG_NO_DYNAMIC:
        # !!! JUST FOR DEBUGING !!!
        print("!!! è­¦å‘Šï¼šæ‚¨å·²é€‰æ‹©ä¸æŠ“å–åŠ¨æ€èŠ‚ç‚¹ !!!")
        AUTOURLS = AUTOFETCH = []

    # åŠ è½½ä¹‹å‰çš„èŠ‚ç‚¹ç»“æœ
    previous_nodes = load_previous_nodes()
    if previous_nodes:
        print(f"å°†é‡æ–°æµ‹è¯• {len(previous_nodes)} ä¸ªä¹‹å‰çš„èŠ‚ç‚¹")
        # å°†ä¹‹å‰çš„èŠ‚ç‚¹æ·»åŠ åˆ°æºåˆ—è¡¨ä¸­ï¼ˆä½œä¸ºå†…å­˜ä¸­çš„æºï¼‰
        # è¿™æ ·å®ƒä»¬ä¼šå’Œæ–°é‡‡é›†çš„èŠ‚ç‚¹ä¸€èµ·è¢«å¤„ç†

    print("æ­£åœ¨ç”ŸæˆåŠ¨æ€é“¾æ¥...")
    for auto_fun in AUTOURLS:
        print("æ­£åœ¨ç”Ÿæˆ '"+auto_fun.__name__+"'... ", end='', flush=True)
        try: url = auto_fun()
        except requests.exceptions.RequestException: print("å¤±è´¥ï¼")
        except: print("é”™è¯¯ï¼š");traceback.print_exc()
        else:
            if url:
                if isinstance(url, str):
                    sources.append(url)
                elif isinstance(url, (list, tuple, set)):
                    sources.extend(url)
                print("æˆåŠŸï¼")
            else: print("è·³è¿‡ï¼")
    print("æ­£åœ¨æ•´ç†é“¾æ¥...")
    sources_final: Union[Set[str], List[str]] = set()
    airports: Set[str] = set()
    for source in sources:
        if source == 'EOF': break
        if not source: continue
        if source[0] == '#': continue
        sub = source
        if sub[0] == '!':
            if LOCAL: continue
            sub = sub[1:]
        if sub[0] == '*':
            isairport = True
            sub = sub[1:]
        else: isairport = False
        if sub[0] == '+':
            tags = sub.split()
            sub = tags.pop()
            sub = ' '.join(tags) + ' ' +raw2fastly(sub)
        else:
            sub = raw2fastly(sub)
        if isairport: airports.add(sub)
        else: sources_final.add(sub)

    if airports:
        print("æ­£åœ¨æŠ“å–æœºåœºåˆ—è¡¨...")
        for sub in airports:
            print("åˆå¹¶ '"+sub+"'... ", end='', flush=True)
            try:
                res = extract(sub)
            except KeyboardInterrupt:
                print("æ­£åœ¨é€€å‡º...")
                break
            except requests.exceptions.RequestException:
                print("åˆå¹¶å¤±è´¥ï¼")
            except: traceback.print_exc()
            else:
                if isinstance(res, int):
                    print(res)
                else:
                    for url in res:
                        sources_final.add(url)
                    print("å®Œæˆï¼")

    print("æ­£åœ¨æ•´ç†é“¾æ¥...")
    sources_final = list(sources_final)
    sources_final.sort()
    sources_obj = [Source(url) for url in (sources_final + AUTOFETCH)]

    print("å¼€å§‹æŠ“å–ï¼")
    threads = [threading.Thread(target=_.get, daemon=True) for _ in sources_obj]
    for thread in threads: thread.start()
    for i in range(len(sources_obj)):
        try:
            for t in range(1, FETCH_TIMEOUT[0]+1):
                print("æŠ“å– '"+sources_obj[i].url+"'... ", end='', flush=True)
                try: threads[i].join(timeout=FETCH_TIMEOUT[1])
                except KeyboardInterrupt:
                    print("æ­£åœ¨é€€å‡º...")
                    FETCH_TIMEOUT = (1, 0)
                    break
                if not threads[i].is_alive(): break
                print(f"{5*t}s")
            if threads[i].is_alive():
                print("è¶…æ—¶ï¼")
                continue
            res = sources_obj[i].content
            if isinstance(res, int):
                if res < 0: print("æŠ“å–å¤±è´¥ï¼")
                else: print(res)
            else:
                print("æ­£åœ¨åˆå¹¶... ", end='', flush=True)
                try:
                    merge(sources_obj[i], sourceId=i)
                except KeyboardInterrupt:
                    print("æ­£åœ¨é€€å‡º...")
                    break
                except:
                    print("å¤±è´¥ï¼")
                    traceback.print_exc()
                else: print("å®Œæˆï¼")
        except KeyboardInterrupt:
            print("æ­£åœ¨é€€å‡º...")
            break
        while exc_queue:
            print(exc_queue.pop(0), file=sys.stderr, flush=True)

    # æ›´æ–°æºå†å²è®°å½•
    print("\næ­£åœ¨æ›´æ–°æºå†å²è®°å½•...")
    for source in sources_obj:
        # è·å–è§„èŒƒåŒ–çš„URLï¼ˆå»æ‰å‚æ•°ç­‰ï¼‰
        url = source.url
        if '#' in url:
            url = url.split('#')[0]
        
        # åˆ¤æ–­æŠ“å–æ˜¯å¦æˆåŠŸ
        success = isinstance(source.content, str) and source.sub is not None
        
        # ç»Ÿè®¡æœ‰æ•ˆèŠ‚ç‚¹æ•°ï¼ˆåœ¨mergedä¸­çš„èŠ‚ç‚¹ï¼‰
        valid_nodes = 0
        if source.sub:
            for p in source.sub:
                try:
                    n = Node(p) if isinstance(p, str) else Node(p)
                    if hash(n) in merged:
                        valid_nodes += 1
                except:
                    pass
        
        update_source_history(source_history, url, success, valid_nodes)
    
    # ä¿å­˜æ›´æ–°åçš„å†å²è®°å½•
    save_source_history(source_history)
    print(f"å·²æ›´æ–° {len(sources_obj)} ä¸ªæºçš„å†å²è®°å½•")

    # åˆå¹¶ä¹‹å‰çš„èŠ‚ç‚¹ï¼ˆç›´æ¥å¤„ç†ï¼Œä¸éœ€è¦é€šè¿‡Sourceå¯¹è±¡ï¼‰
    if previous_nodes:
        print(f"\næ­£åœ¨åˆå¹¶ä¹‹å‰çš„ {len(previous_nodes)} ä¸ªèŠ‚ç‚¹... ", end='', flush=True)
        # ç›´æ¥éå†ä¹‹å‰çš„èŠ‚ç‚¹URLå¹¶åˆå¹¶
        previous_count = 0
        for node_url in previous_nodes:
            try:
                n = Node(node_url)
                n.format_name()
                Node.names.add(n.data['name'])
                hashn = hash(n)
                if hashn not in merged:
                    # åªæœ‰å½“èŠ‚ç‚¹ä¸å­˜åœ¨æ—¶æ‰æ·»åŠ ï¼ˆæ–°é‡‡é›†çš„èŠ‚ç‚¹ä¼˜å…ˆï¼‰
                    merged[hashn] = n
                    previous_count += 1
                    # è®°å½•è¿™ä¸ªèŠ‚ç‚¹æ¥è‡ª"ä¹‹å‰çš„ç»“æœ"ï¼ˆä½¿ç”¨ç‰¹æ®Šçš„sourceId=-1ï¼‰
                    if hashn not in used:
                        used[hashn] = {}
                    used[hashn][-1] = n.name
                # else: èŠ‚ç‚¹å·²å­˜åœ¨ï¼ˆæ–°é‡‡é›†çš„æºä¸­ä¹Ÿæœ‰è¿™ä¸ªèŠ‚ç‚¹ï¼‰ï¼Œä¿ç•™æ–°çš„ï¼Œä¸åšä»»ä½•æ“ä½œ
            except Exception as e:
                # å¿½ç•¥æ— æ³•è§£æçš„èŠ‚ç‚¹
                pass
        print(f"å®Œæˆï¼æ–°å¢ {previous_count} ä¸ªä¹‹å‰çš„èŠ‚ç‚¹ï¼ˆå»é‡åï¼‰")

    if STOP:
        merged = {}
        for nid, nd in enumerate(STOP_FAKE_NODES.splitlines()):
            merged[nid] = Node(nd)

    # æµ‹è¯•èŠ‚ç‚¹å»¶è¿Ÿå¹¶è¿‡æ»¤æ— æ•ˆèŠ‚ç‚¹
    if merged and not STOP:
        print("\n" + "="*60)
        # åªä¿ç•™å»¶è¿Ÿå°äº1000msçš„èŠ‚ç‚¹
        # ä½¿ç”¨å¤šä¸ªæµ‹è¯•URLå’Œå¹¶å‘æµ‹è¯•ï¼Œæé«˜æµ‹è¯•é€Ÿåº¦å’ŒæˆåŠŸç‡
        filtered = filter_nodes_by_delay(merged, max_delay=1.0, max_workers=50)
        print("="*60)
        
        # å¦‚æœæµ‹è¯•å¤±è´¥è¿”å› Noneï¼Œä½¿ç”¨ä¸Šæ¬¡çš„èŠ‚ç‚¹
        if filtered is None:
            print("èŠ‚ç‚¹æµ‹è¯•å¤±è´¥ï¼Œå°†ä½¿ç”¨ä¸Šæ¬¡çš„æœ‰æ•ˆèŠ‚ç‚¹")
            # åªä¿ç•™ä¹‹å‰ç»“æœä¸­çš„èŠ‚ç‚¹ï¼ˆsourceId=-1 çš„èŠ‚ç‚¹ï¼‰
            previous_only = {}
            for hashp, node in merged.items():
                if hashp in used and -1 in used[hashp]:
                    previous_only[hashp] = node
            if previous_only:
                print(f"ä¿ç•™ {len(previous_only)} ä¸ªä¸Šæ¬¡çš„èŠ‚ç‚¹")
                merged = previous_only
            else:
                print("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°ä¸Šæ¬¡çš„èŠ‚ç‚¹ï¼Œå°†ä¿ç•™æ‰€æœ‰èŠ‚ç‚¹")
        else:
            merged = filtered

    print("\næ­£åœ¨å†™å‡º V2Ray è®¢é˜…...")
    txt = ""
    unsupports = 0
    for hashp, p in merged.items():
        try:
            if hashp in used:
                # æ³¨æ„ï¼šè¿™ä¸€æ­¥ä¹Ÿä¼šå½±å“åˆ°ä¸‹æ–¹çš„ Clash è®¢é˜…ï¼Œä¸ç”¨å†æ‰§è¡Œä¸€éï¼
                p.data['name'] = ','.join([str(_) for _ in sorted(list(used[hash(p)]))])+'|'+p.data['name']
            if p.supports_ray():
                try:
                    txt += p.url + '\n'
                except UnsupportedType as e:
                    print(f"ä¸æ”¯æŒçš„ç±»å‹ï¼š{e}")
            else: unsupports += 1
        except: traceback.print_exc()
    for p in unknown:
        txt += p+'\n'
    print(f"å…±æœ‰ {len(merged)-unsupports} ä¸ªæ­£å¸¸èŠ‚ç‚¹ï¼Œ{len(unknown)} ä¸ªæ— æ³•è§£æçš„èŠ‚ç‚¹ï¼Œå…±",
            len(merged)+len(unknown),f"ä¸ªã€‚{unsupports} ä¸ªèŠ‚ç‚¹ä¸è¢« V2Ray æ”¯æŒã€‚")

    with open("list_raw.txt", 'w', encoding="utf-8") as f:
        f.write(txt)
    with open("list.txt", 'w', encoding="utf-8") as f:
        f.write(b64encodes(txt))
    print("å†™å‡ºå®Œæˆï¼")

    with open("config.yml", encoding="utf-8") as f:
        conf: Dict[str, Any] = yaml.full_load(f)
    
    rules: Dict[str, str] = {}
    if DEBUG_NO_ADBLOCK:
        # !!! JUST FOR DEBUGING !!!
        print("!!! è­¦å‘Šï¼šæ‚¨å·²å…³é—­å¯¹ Adblock è§„åˆ™çš„æŠ“å– !!!")
    else:
        merge_adblock(conf['proxy-groups'][-2]['name'], rules)

    snip_conf: Dict[str, Dict[str, Any]] = {}
    ctg_nodes: Dict[str, List[Node.DATA_TYPE]] = {}
    ctg_nodes_meta: Dict[str, List[Node.DATA_TYPE]] = {}
    categories: Dict[str, List[str]] = {}
    try:
        with open("snippets/_config.yml", encoding="utf-8") as f:
            snip_conf = yaml.full_load(f)
    except (OSError, yaml.error.YAMLError):
        print("ç‰‡æ®µé…ç½®è¯»å–å¤±è´¥ï¼š")
        traceback.print_exc()
    else:
        print("æ­£åœ¨æŒ‰åœ°åŒºåˆ†ç±»èŠ‚ç‚¹...")
        categories = snip_conf['categories']
        for ctg in categories:
            ctg_nodes[ctg] = []
            ctg_nodes_meta[ctg] = []
        for node in merged.values():
            if node.supports_meta():
                ctgs: List[str] = []
                for ctg, keys in categories.items():
                    for key in keys:
                        if key in node.name:
                            ctgs.append(ctg)
                            break
                    if ctgs and keys[-1] == 'OVERALL':
                        break
                if len(ctgs) == 1:
                    if node.supports_clash():
                        ctg_nodes[ctgs[0]].append(node.clash_data)
                    ctg_nodes_meta[ctgs[0]].append(node.clash_data)
        for ctg, proxies in ctg_nodes.items():
            with open("snippets/nodes_"+ctg+".yml", 'w', encoding="utf-8") as f:
                yaml.dump({'proxies': proxies}, f, allow_unicode=True)
        for ctg, proxies in ctg_nodes_meta.items():
            with open("snippets/nodes_"+ctg+".meta.yml", 'w', encoding="utf-8") as f:
                yaml.dump({'proxies': proxies}, f, allow_unicode=True)

    print("æ­£åœ¨å†™å‡º Clash & Meta è®¢é˜…...")
    keywords: List[str] = []
    suffixes: List[str] = []
    match_rule = None
    for rule in conf['rules']:
        rule: str
        tmp = rule.strip().split(',')
        if len(tmp) == 2 and tmp[0] == 'MATCH':
            match_rule = rule
            break
        if len(tmp) == 3:
            rtype, rargument, rpolicy = tmp
            if rtype == 'DOMAIN-KEYWORD':
                keywords.append(rargument)
            elif rtype == 'DOMAIN-SUFFIX':
                suffixes.append(rargument)
        elif len(tmp) == 4:
            rtype, rargument, rpolicy, rresolve = tmp
            rpolicy += ','+rresolve
        else: print("è§„åˆ™ '"+rule+"' æ— æ³•è¢«è§£æï¼"); continue
        for kwd in keywords:
            if kwd in rargument and kwd != rargument:
                print(rargument, "å·²è¢« KEYWORD", kwd, "å‘½ä¸­")
                break
        else:
            for sfx in suffixes:
                if ('.'+rargument).endswith('.'+sfx) and sfx != rargument:
                    print(rargument, "å·²è¢« SUFFIX", sfx, "å‘½ä¸­")
                    break
            else:
                k = rtype+','+rargument
                if k not in rules:
                    rules[k] = rpolicy
    conf['rules'] = [','.join(_) for _ in rules.items()]+[match_rule]

    # Clash & Meta
    global_fp: Optional[str] = conf.get('global-client-fingerprint', None)
    proxies: List[Node.DATA_TYPE] = []
    proxies_meta: List[Node.DATA_TYPE] = []
    ctg_base: Dict[str, Any] = conf['proxy-groups'][3].copy()
    names_clash: Union[Set[str], List[str]] = set()
    names_clash_meta: Union[Set[str], List[str]] = set()
    for p in merged.values():
        if p.supports_meta():
            if ('client-fingerprint' in p.data and
                    p.data['client-fingerprint'] == global_fp):
                del p.data['client-fingerprint']
            proxies_meta.append(p.clash_data)
            names_clash_meta.add(p.data['name'])
            if p.supports_clash():
                proxies.append(p.clash_data)
                names_clash.add(p.data['name'])
    names_clash = list(names_clash)
    names_clash_meta = list(names_clash_meta)
    conf_meta = copy.deepcopy(conf)

    # Clash
    conf['proxies'] = proxies
    for group in conf['proxy-groups']:
        if not group['proxies']:
            group['proxies'] = names_clash
    if snip_conf:
        conf['proxy-groups'][-1]['proxies'] = []
        ctg_selects: List[str] = conf['proxy-groups'][-1]['proxies']
        ctg_disp: Dict[str, str] = snip_conf['categories_disp']
        for ctg, payload in ctg_nodes.items():
            if ctg in ctg_disp:
                disp = ctg_base.copy()
                disp['name'] = ctg_disp[ctg]
                if not payload: disp['proxies'] = ['REJECT']
                else: disp['proxies'] = [_['name'] for _ in payload]
                conf['proxy-groups'].append(disp)
                ctg_selects.append(disp['name'])
    try:
        dns_mode: Optional[str] = conf['dns']['enhanced-mode']
    except:
        dns_mode: Optional[str] = None
    else:
        conf['dns']['enhanced-mode'] = 'fake-ip'
    with open("list.yml", 'w', encoding="utf-8") as f:
        f.write(datetime.datetime.now().strftime('# Update: %Y-%m-%d %H:%M\n'))
        f.write(yaml.dump(conf, allow_unicode=True).replace('!!str ',''))
    with open("snippets/nodes.yml", 'w', encoding="utf-8") as f:
        f.write(yaml.dump({'proxies': proxies}, allow_unicode=True).replace('!!str ',''))

    # Meta
    conf = conf_meta
    conf['proxies'] = proxies_meta
    for group in conf['proxy-groups']:
        if not group['proxies']:
            group['proxies'] = names_clash_meta
    if snip_conf:
        conf['proxy-groups'][-1]['proxies'] = []
        ctg_selects: List[str] = conf['proxy-groups'][-1]['proxies']
        ctg_disp: Dict[str, str] = snip_conf['categories_disp']
        for ctg, payload in ctg_nodes_meta.items():
            if ctg in ctg_disp:
                disp = ctg_base.copy()
                disp['name'] = ctg_disp[ctg]
                if not payload: disp['proxies'] = ['REJECT']
                else: disp['proxies'] = [_['name'] for _ in payload]
                conf['proxy-groups'].append(disp)
                ctg_selects.append(disp['name'])
    if dns_mode:
        conf['dns']['enhanced-mode'] = dns_mode
    with open("list.meta.yml", 'w', encoding="utf-8") as f:
        f.write(datetime.datetime.now().strftime('# Update: %Y-%m-%d %H:%M\n'))
        f.write(yaml.dump(conf, allow_unicode=True).replace('!!str ',''))
    with open("snippets/nodes.meta.yml", 'w', encoding="utf-8") as f:
        f.write(yaml.dump({'proxies': proxies_meta}, allow_unicode=True).replace('!!str ',''))

    if snip_conf:
        print("æ­£åœ¨å†™å‡ºé…ç½®ç‰‡æ®µ...")
        name_map: Dict[str, str] = snip_conf['name-map']
        snippets: Dict[str, List[str]] = {}
        for rpolicy in name_map.values(): snippets[rpolicy] = []
        for rule, rpolicy in rules.items():
            if ',' in rpolicy: rpolicy = rpolicy.split(',')[0]
            if rpolicy in name_map:
                snippets[name_map[rpolicy]].append(rule)
        for name, payload in snippets.items():
            with open("snippets/"+name+".yml", 'w', encoding="utf-8") as f:
                yaml.dump({'payload': payload}, f, allow_unicode=True)

    print("æ­£åœ¨å†™å‡ºç»Ÿè®¡ä¿¡æ¯...")
    out = "åºå·,é“¾æ¥,èŠ‚ç‚¹æ•°\n"
    for i, source in enumerate(sources_obj):
        out += f"{i},{source.url},"
        try: out += f"{len(source.sub)}"
        except: out += '0'
        out += '\n'
    out += f"\næ€»è®¡,,{len(merged)}\n"
    open("list_result.csv",'w').write(out)

    # æ¸…ç†æ— æ•ˆçš„è®¢é˜…æºï¼ˆè¿ç»­7å¤©å¤±è´¥æˆ–æ— æœ‰æ•ˆä»£ç†ï¼‰
    print("\næ­£åœ¨æ£€æŸ¥æ— æ•ˆè®¢é˜…æº...")
    deleted = cleanup_invalid_sources(source_history)
    if deleted:
        print(f"å·²æ¸…ç† {len(deleted)} ä¸ªæ— æ•ˆè®¢é˜…æºï¼š")
        for original_line, url, reason in deleted:
            print(f"  - {url[:60]}... ({reason})")
    else:
        print("æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ— æ•ˆè®¢é˜…æº")

    print("\nå†™å‡ºå®Œæˆï¼")

if __name__ == '__main__':
    from dynamic import AUTOURLS, AUTOFETCH # type: ignore
    main()
